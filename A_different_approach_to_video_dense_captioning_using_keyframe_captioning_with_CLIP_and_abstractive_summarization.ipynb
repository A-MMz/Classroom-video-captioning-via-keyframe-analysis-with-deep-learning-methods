{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "8eEt-CWCejPV",
        "ENlJ78N9psnC"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "68b05a52c5944f17a89415c287655f20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3f294ec0bbbe4e1296a046e05ec69349",
              "IPY_MODEL_b729ec41c14e48379e2c30d9f7c45958",
              "IPY_MODEL_3bea721c7acc4e87b8d5f9b43f778e1f"
            ],
            "layout": "IPY_MODEL_0eed5a353dfe4a2e8f15a592320871fa"
          }
        },
        "3f294ec0bbbe4e1296a046e05ec69349": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_049a37df1c9d4d72961616c3d7ecf91a",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_b7c07c50ae6b4f91a2f88de6aa66fa08",
            "value": "100%"
          }
        },
        "b729ec41c14e48379e2c30d9f7c45958": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_98f924e6ee2446f5980e9d3c255eab13",
            "max": 14808437,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5a4bdd620caf452b80ea69bbde8db148",
            "value": 14808437
          }
        },
        "3bea721c7acc4e87b8d5f9b43f778e1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_376b75e4cf9646eb95e800cf7b4ed7f0",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_cf40a932d5c44e4b9c2689b66899231b",
            "value": " 14.1M/14.1M [00:00&lt;00:00, 33.7MB/s]"
          }
        },
        "0eed5a353dfe4a2e8f15a592320871fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "049a37df1c9d4d72961616c3d7ecf91a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7c07c50ae6b4f91a2f88de6aa66fa08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "98f924e6ee2446f5980e9d3c255eab13": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a4bdd620caf452b80ea69bbde8db148": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "376b75e4cf9646eb95e800cf7b4ed7f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf40a932d5c44e4b9c2689b66899231b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Key Frames Extraction"
      ],
      "metadata": {
        "id": "pqIM9hJkjSRB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HjaLdMBcYqgj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import cv2\n",
        "import datetime\n",
        "import torch\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cap = cv2.VideoCapture('v18.mp4') \n",
        "\n",
        "arr = np.empty((0, 1944), int)   #initializing 1944 dimensional array to store 'flattened' color histograms\n",
        "D=dict()   #to store the original frame (array)\n",
        "count=0    #counting the number of frames\n",
        "start_time = time.time()\n",
        "while cap.isOpened():\n",
        "    \n",
        "    # Read the video file.\n",
        "    ret, frame = cap.read()\n",
        "    \n",
        "    # If we got frames.\n",
        "    if ret == True:\n",
        "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  #since cv reads frame in bgr order so rearraning to get frames in rgb order\n",
        "        D[count] = frame_rgb   #storing each frame (array) to D , so that we can identify key frames later \n",
        "        \n",
        "        #dividing a frame into 3*3 i.e 9 blocks\n",
        "        height, width, channels = frame_rgb.shape\n",
        "\n",
        "        if height % 3 == 0:\n",
        "            h_chunk = int(height/3)\n",
        "        else:\n",
        "            h_chunk = int(height/3) + 1\n",
        "\n",
        "        if width % 3 == 0:\n",
        "            w_chunk = int(width/3)\n",
        "        else:\n",
        "            w_chunk = int(width/3) + 1\n",
        "\n",
        "        h=0\n",
        "        w= 0 \n",
        "        feature_vector = []\n",
        "        for a in range(1,4):\n",
        "            h_window = h_chunk*a\n",
        "            for b in range(1,4):\n",
        "                frame = frame_rgb[h : h_window, w : w_chunk*b , :]\n",
        "                hist = cv2.calcHist(frame, [0, 1, 2], None, [6, 6, 6], [0, 256, 0, 256, 0, 256])#finding histograms for each block  \n",
        "                hist1= hist.flatten()  #flatten the hist to one-dimensinal vector \n",
        "                feature_vector += list(hist1)\n",
        "                w = w_chunk*b\n",
        "                \n",
        "            h = h_chunk*a\n",
        "            w= 0\n",
        "\n",
        "                \n",
        "        arr =np.vstack((arr, feature_vector )) #appending each one-dimensinal vector to generate N*M matrix (where N is number of frames\n",
        "          #and M is 1944) \n",
        "        count+=1\n",
        "    else:\n",
        "        break\n",
        "\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "\n",
        "final_arr = arr.transpose() #transposing so that i will have all frames in columns i.e M*N dimensional matrix \n",
        "#where M is 1944 and N is number of frames\n",
        "print(final_arr.shape)\n",
        "print(count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q39WcNhIY1Tn",
        "outputId": "04961d06-f57b-41a2-b4f5-ecc84fd28a4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 5.644730567932129 seconds ---\n",
            "(1944, 570)\n",
            "570\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.sparse import csc_matrix\n",
        "from scipy.sparse.linalg import svds, eigs\n",
        "A = csc_matrix(final_arr, dtype=float)\n",
        "\n",
        "#top 63 singular values from 76082 to 508\n",
        "u, s, vt = svds(A, k = 63)"
      ],
      "metadata": {
        "id": "n-FiGgmnY1P0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(u.shape, s.shape, vt.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIYGRheRY1Mb",
        "outputId": "c13e9a10-d67d-4de5-cbd9-584898117a8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1944, 63) (63,) (63, 570)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(s))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atkiijN1Y1KH",
        "outputId": "b0dc8aa5-5f23-4d33-ec20-8d438ec7c56f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[46.037526249144136, 46.56319905092684, 46.832172190903506, 47.782836063356434, 48.76244153710375, 49.95496799661359, 50.854404855036805, 52.416194324050664, 53.57531537139949, 55.37421639070441, 55.865763166357524, 57.61659739349478, 58.71967948501821, 61.05288377957271, 62.936405947657775, 63.90003166713214, 66.12001789399193, 68.71991574223081, 69.99154125925496, 71.98601857584845, 73.65127928891839, 74.1959782503641, 77.32922820770243, 78.43901293025912, 83.26829984390396, 87.27639540882306, 87.85312400511836, 90.11493849664488, 92.20384530979874, 95.79652962785829, 107.79924641938736, 108.67886587428218, 112.96949701101029, 114.42932887513881, 119.2193755803186, 122.98501149796529, 130.67668419370358, 141.1236938303202, 147.6376899468303, 156.3250037615294, 164.44946816822937, 176.76388834430236, 191.3242465275119, 193.95119064545653, 207.51372402542734, 220.0569253254723, 252.54846635708228, 268.74338828150087, 306.8042303688005, 386.30061925944796, 414.34767763880654, 422.5128774381331, 530.5238035553701, 547.7421079767721, 578.0500870514396, 792.2316921707538, 1008.6015575017509, 1197.0034778709662, 1790.620305683393, 3318.2328271692204, 20122.919045187078, 23066.54518189377, 42709.004174148446]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "v1_t = vt.transpose()\n",
        "\n",
        "projections = v1_t @ np.diag(s) #the column vectors i.e the frame histogram data has been projected onto the orthonormal basis \n",
        "#formed by vectors of the left singular matrix u .The coordinates of the frames in this space are given by v1_t @ np.diag(s)\n",
        "#So we can see that , now we need only 63 dimensions to represent each column/frame \n",
        "print(projections.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3aCwF3zY1He",
        "outputId": "62d29952-5b58-4fe9-9acf-ec54c64c8c10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(570, 63)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#dynamic clustering of projected frame histograms to find which all frames are similar i.e make shots\n",
        "f=projections\n",
        "C = dict() #to store frames in respective cluster\n",
        "for i in range(f.shape[0]):\n",
        "    C[i] = np.empty((0,63), int)\n",
        "    \n",
        "#adding first two projected frames in first cluster i.e Initializaton    \n",
        "C[0] = np.vstack((C[0], f[0]))   \n",
        "C[0] = np.vstack((C[0], f[1]))\n",
        "\n",
        "E = dict() #to store centroids of each cluster\n",
        "for i in range(projections.shape[0]):\n",
        "    E[i] = np.empty((0,63), int)\n",
        "    \n",
        "E[0] = np.mean(C[0], axis=0) #finding centroid of C[0] cluster\n",
        "\n",
        "count = 0\n",
        "for i in range(2,f.shape[0]):\n",
        "    similarity = np.dot(f[i], E[count])/( (np.dot(f[i],f[i]) **.5) * (np.dot(E[count], E[count]) ** .5)) #cosine similarity\n",
        "    #this metric is used to quantify how similar is one vector to other. The maximum value is 1 which indicates they are same\n",
        "    #and if the value is 0 which indicates they are orthogonal nothing is common between them.\n",
        "    #Here we want to find similarity between each projected frame and last cluster formed chronologically. \n",
        "     \n",
        "    \n",
        "    if similarity < 0.9: #if the projected frame and last cluster formed  are not similar upto 0.9 cosine value then \n",
        "                         #we assign this data point to newly created cluster and find centroid \n",
        "                         #We checked other thresholds also like 0.85, 0.875, 0.95, 0.98\n",
        "                        #but 0.9 looks okay because as we go below then we get many key-frames for similar event and \n",
        "                        #as we go above we have lesser number of key-frames thus missed some events. So, 0.9 seems optimal.\n",
        "                        \n",
        "        count+=1         \n",
        "        C[count] = np.vstack((C[count], f[i])) \n",
        "        E[count] = np.mean(C[count], axis=0)   \n",
        "    else:  #if they are similar then assign this data point to last cluster formed and update the centroid of the cluster\n",
        "        C[count] = np.vstack((C[count], f[i])) \n",
        "        E[count] = np.mean(C[count], axis=0)         "
      ],
      "metadata": {
        "id": "NUf9fN-lY1Ef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b = []  #find the number of data points in each cluster formed.\n",
        "\n",
        "#We can assume that sparse clusters indicates \n",
        "#transition between shots so we will ignore these frames which lies in such clusters and wherever the clusters are densely populated indicates they form shots\n",
        "#and we can take the last element of these shots to summarise that particular shot\n",
        "\n",
        "for i in range(f.shape[0]):\n",
        "    b.append(C[i].shape[0])\n",
        "\n",
        "last = b.index(0)  #where we find 0 in b indicates that all required clusters have been formed , so we can delete these from C\n",
        "b1=b[:last ] #The size of each cluster."
      ],
      "metadata": {
        "id": "dX3y8VwgY1A-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = [idx for idx, val in enumerate(b1) if val >= 25] #so i am assuming any dense cluster with atleast 25 frames is eligible to \n",
        "#make shot.\n",
        "print(len(res)) #so total 25 shots with 46 (71-25) cuts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jm4XCRYpY92x",
        "outputId": "07d08e13-ccc4-48da-b27f-8b5a19985ab7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GG = C #copying the elements of C to GG, the purpose of  the below code is to label each cluster so later \n",
        "#it would be easier to identify frames in each cluster\n",
        "for i in range(last):\n",
        "    p1= np.repeat(i, b1[i]).reshape(b1[i],1)\n",
        "    GG[i] = np.hstack((GG[i],p1))"
      ],
      "metadata": {
        "id": "q5fch_bzY90p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#the purpose of the below code is to append each cluster to get multidimensional array of dimension N*64, N is number of frames\n",
        "F=  np.empty((0,64), int) \n",
        "for i in range(last):\n",
        "    F = np.vstack((F,GG[i]))"
      ],
      "metadata": {
        "id": "65gCfLqtY9yo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#converting F (multidimensional array)  to dataframe\n",
        "\n",
        "colnames = []\n",
        "for i in range(1, 65):\n",
        "    col_name = \"v\" + str(i)\n",
        "    colnames+= [col_name]\n",
        "print(colnames)\n",
        "\n",
        "df = pd.DataFrame(F, columns= colnames)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXxBRTioZBca",
        "outputId": "60d40d74-089a-4bdb-afc4-ba0d9a85a03a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['v1', 'v2', 'v3', 'v4', 'v5', 'v6', 'v7', 'v8', 'v9', 'v10', 'v11', 'v12', 'v13', 'v14', 'v15', 'v16', 'v17', 'v18', 'v19', 'v20', 'v21', 'v22', 'v23', 'v24', 'v25', 'v26', 'v27', 'v28', 'v29', 'v30', 'v31', 'v32', 'v33', 'v34', 'v35', 'v36', 'v37', 'v38', 'v39', 'v40', 'v41', 'v42', 'v43', 'v44', 'v45', 'v46', 'v47', 'v48', 'v49', 'v50', 'v51', 'v52', 'v53', 'v54', 'v55', 'v56', 'v57', 'v58', 'v59', 'v60', 'v61', 'v62', 'v63', 'v64']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['v64']= df['v64'].astype(int)  #converting the cluster level from float type to i"
      ],
      "metadata": {
        "id": "vujNGnedZBZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 =  df[df.v64.isin(res)]   #filter only those frames which are eligible to be a part of shot or filter those frames who are\n",
        "#part of required clusters that have more than 25 frames in it"
      ],
      "metadata": {
        "id": "WDgaS_UOZEYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new = df1.groupby('v64').tail(1)['v64'] #For each cluster /group take its last element which summarize the shot i.e key-frame"
      ],
      "metadata": {
        "id": "qWLRl3TeZEVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new1 = new.index #finding key-frames (frame number so that we can go back get the original picture)"
      ],
      "metadata": {
        "id": "yBoUrO_7ZESv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#output the frames in png format\n",
        "images= []\n",
        "for c in new1:\n",
        "    frame_rgb1 = cv2.cvtColor(D[c], cv2.COLOR_RGB2BGR) #since cv consider image in BGR order\n",
        "    frame_num_chr = str(c)\n",
        "    file_name = 'frame'+ frame_num_chr +'.png'\n",
        "    images.append(frame_num_chr)\n",
        "    cv2.imwrite(file_name, frame_rgb1)"
      ],
      "metadata": {
        "id": "Dha1Q2FhZEQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fileNames=[]\n",
        "for c in new1:\n",
        "  frame_rgb1 = cv2.cvtColor(D[c], cv2.COLOR_RGB2BGR) #since cv consider image in BGR order\n",
        "  frame_num_chr = str(c)\n",
        "  fileNames.append('frame'+ frame_num_chr)\n",
        "fileNames"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5YfMkscq_Ts",
        "outputId": "cf06615b-26e6-4518-fe92-af172a796db0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['frame157', 'frame362', 'frame516', 'frame569']"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fps=cap.get(cv2.CAP_PROP_FPS)"
      ],
      "metadata": {
        "id": "kOlNbegVh-uJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Frames per second using cap.get(cv2.CAP_PROP_FPS) : {0}\".format(fps))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eS-KXtmOiCCB",
        "outputId": "cd4688f7-80dd-420e-e827-d9352fe97a1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frames per second using cap.get(cv2.CAP_PROP_FPS) : 30.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "frame_duration = (1/fps)\n",
        "frame_duration"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TafGKNmHhEeA",
        "outputId": "4333cb48-32f0-4575-e3d7-83a45b4cba6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.03333333333333333"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "diccionario = {}\n",
        "contador=0\n",
        "for k in new1:\n",
        "  a1=[]\n",
        "  a1.append(datetime.datetime.fromtimestamp(round(frame_duration*k, 2)).strftime('%H:%M:%S:%f'))\n",
        "  diccionario[fileNames[contador]] = a1\n",
        "  contador=contador + 1"
      ],
      "metadata": {
        "id": "Xb5Y2ZUgiik-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diccionario"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WfXHXLRHiRQX",
        "outputId": "18580dcb-6422-41c2-e245-5cd97ec24108"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'frame157': ['00:00:05:230000'],\n",
              " 'frame362': ['00:00:12:070000'],\n",
              " 'frame516': ['00:00:17:200000'],\n",
              " 'frame569': ['00:00:18:970000']}"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Crops with Object Detection"
      ],
      "metadata": {
        "id": "tl4h2XJeN-r4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')  # or yolov5n - yolov5x6, custom"
      ],
      "metadata": {
        "id": "Msm4bLWl6joa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "68b05a52c5944f17a89415c287655f20",
            "3f294ec0bbbe4e1296a046e05ec69349",
            "b729ec41c14e48379e2c30d9f7c45958",
            "3bea721c7acc4e87b8d5f9b43f778e1f",
            "0eed5a353dfe4a2e8f15a592320871fa",
            "049a37df1c9d4d72961616c3d7ecf91a",
            "b7c07c50ae6b4f91a2f88de6aa66fa08",
            "98f924e6ee2446f5980e9d3c255eab13",
            "5a4bdd620caf452b80ea69bbde8db148",
            "376b75e4cf9646eb95e800cf7b4ed7f0",
            "cf40a932d5c44e4b9c2689b66899231b"
          ]
        },
        "outputId": "8b8a8783-11ee-465a-925b-9165d5d53460"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/hub.py:267: UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour\n",
            "  \"You are about to download and run code from an untrusted repository. In a future release, this won't \"\n",
            "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n",
            "INFO:yolov5:\u001b[31m\u001b[1mrequirements:\u001b[0m YOLOv5 requirement \"ipython\" not found, attempting AutoUpdate...\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m YOLOv5 requirement \"ipython\" not found, attempting AutoUpdate...\n",
            "INFO:yolov5:Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (7.9.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython) (4.4.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython) (0.2.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython) (57.4.0)\n",
            "Collecting jedi>=0.10\n",
            "  Downloading jedi-0.18.1-py2.py3-none-any.whl (1.6 MB)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython) (5.1.1)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython) (4.8.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython) (2.0.10)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython) (2.6.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.10->ipython) (0.8.3)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython) (0.2.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython) (0.7.0)\n",
            "Installing collected packages: jedi\n",
            "Successfully installed jedi-0.18.1\n",
            "\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (7.9.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython) (4.4.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython) (0.2.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython) (57.4.0)\n",
            "Collecting jedi>=0.10\n",
            "  Downloading jedi-0.18.1-py2.py3-none-any.whl (1.6 MB)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython) (5.1.1)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython) (4.8.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython) (2.0.10)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython) (2.6.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.10->ipython) (0.8.3)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython) (0.2.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython) (0.7.0)\n",
            "Installing collected packages: jedi\n",
            "Successfully installed jedi-0.18.1\n",
            "\n",
            "INFO:yolov5:\u001b[31m\u001b[1mrequirements:\u001b[0m 1 package updated per /root/.cache/torch/hub/ultralytics_yolov5_master/requirements.txt\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m ‚ö†Ô∏è \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m 1 package updated per /root/.cache/torch/hub/ultralytics_yolov5_master/requirements.txt\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m ‚ö†Ô∏è \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n",
            "INFO:yolov5:YOLOv5 üöÄ 2022-10-26 Python-3.7.15 torch-1.12.1+cu113 CUDA:0 (Tesla T4, 15110MiB)\n",
            "\n",
            "YOLOv5 üöÄ 2022-10-26 Python-3.7.15 torch-1.12.1+cu113 CUDA:0 (Tesla T4, 15110MiB)\n",
            "\n",
            "INFO:yolov5:Downloading https://github.com/ultralytics/yolov5/releases/download/v6.2/yolov5s.pt to yolov5s.pt...\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v6.2/yolov5s.pt to yolov5s.pt...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/14.1M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "68b05a52c5944f17a89415c287655f20"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:yolov5:\n",
            "\n",
            "INFO:yolov5:Fusing layers... \n",
            "Fusing layers... \n",
            "INFO:yolov5:YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
            "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
            "INFO:yolov5:Adding AutoShape... \n",
            "Adding AutoShape... \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.isdir('data'):\n",
        "  %rm -rf 'data'\n",
        "\n",
        "for img in diccionario:\n",
        "  img_path = (img+'.png')\n",
        "  result = model(img_path)\n",
        "  dir = result.crop(save=True, save_dir='data/'+img)\n",
        "\n",
        "  if os.path.isdir('data/'+img+'/crops/person'):\n",
        "    diccionario[img].append(os.listdir('data/'+img+'/crops/person'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmjKw4adB7jm",
        "outputId": "7ea8b7c1-a34b-4e71-9d7a-5bd03d612923"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:yolov5:Saved 1 image to \u001b[1mdata/frame157\u001b[0m\n",
            "Saved 1 image to \u001b[1mdata/frame157\u001b[0m\n",
            "INFO:yolov5:Saved results to data/frame157\n",
            "\n",
            "Saved results to data/frame157\n",
            "\n",
            "INFO:yolov5:Saved 1 image to \u001b[1mdata/frame362\u001b[0m\n",
            "Saved 1 image to \u001b[1mdata/frame362\u001b[0m\n",
            "INFO:yolov5:Saved results to data/frame362\n",
            "\n",
            "Saved results to data/frame362\n",
            "\n",
            "INFO:yolov5:Saved 1 image to \u001b[1mdata/frame516\u001b[0m\n",
            "Saved 1 image to \u001b[1mdata/frame516\u001b[0m\n",
            "INFO:yolov5:Saved results to data/frame516\n",
            "\n",
            "Saved results to data/frame516\n",
            "\n",
            "INFO:yolov5:Saved 1 image to \u001b[1mdata/frame569\u001b[0m\n",
            "Saved 1 image to \u001b[1mdata/frame569\u001b[0m\n",
            "INFO:yolov5:Saved results to data/frame569\n",
            "\n",
            "Saved results to data/frame569\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo de keyframe en diccionario con crops\n",
        "diccionario['frame157']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYg5QNZSOJzA",
        "outputId": "b3ec2819-7d08-4353-ea96-7205d2960e17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['00:00:05:230000',\n",
              " ['frame1574.jpg',\n",
              "  'frame1575.jpg',\n",
              "  'frame157.jpg',\n",
              "  'frame1579.jpg',\n",
              "  'frame1577.jpg',\n",
              "  'frame1573.jpg',\n",
              "  'frame1578.jpg',\n",
              "  'frame1576.jpg',\n",
              "  'frame1572.jpg']]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image Classification"
      ],
      "metadata": {
        "id": "8eEt-CWCejPV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -o saved_model.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQssxLN-iBK6",
        "outputId": "4913f6d9-68e8-47a9-e0e5-c50430643629"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open saved_model.zip, saved_model.zip.zip or saved_model.zip.ZIP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.load_model('content/saved_model/my_model')"
      ],
      "metadata": {
        "id": "Zega4_vYet-j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "outputId": "0c9f3f74-0070-4e7d-de9e-2ab67984f804"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-08c5dedc3338>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'content/saved_model/my_model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'No file or directory found at {filepath_str}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: No file or directory found at content/saved_model/my_model"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_size = (180, 180)\n",
        "\n",
        "img = tf.keras.preprocessing.image.load_img(\"/content/data/frame157/crops/person/frame1572.jpg\", target_size=image_size)"
      ],
      "metadata": {
        "id": "yoBRYYB9iv0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img"
      ],
      "metadata": {
        "id": "o6_vJNmTln2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
        "img_array = tf.expand_dims(img_array, 0)  # Create batch axis\n",
        "\n",
        "predictions = model.predict(img_array)\n",
        "score = predictions[0]\n",
        "print(score)\n",
        "print(\"This image is %.2f percent raising_hand and %.2f percent sleeping.\"\n",
        "    % (100 * (1 - score), 100 * score))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wcOLQGWjyp9",
        "outputId": "c6506df6-7e09-4a8f-a584-7a77f4323498"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 18ms/step\n",
            "[    0.99992]\n",
            "This image is 0.01 percent raising_hand and 99.99 percent sleeping.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdBjRnWqLwWP"
      },
      "source": [
        "# Inference notenook for [CLIP prefix captioning](https://github.com/rmokady/CLIP_prefix_caption/)\n",
        "\n",
        "Disclaimer: the authors do not own any rights for the code or data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRfpGaz27IWs",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "070c9061-50f4-405e-9e2a-56eb2dce446c"
      },
      "source": [
        "#@title Install\n",
        "!pip install transformers\n",
        "! pip install git+https://github.com/openai/CLIP.git\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.23.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.13.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.9.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-z8wsjlpn\n",
            "  Running command git clone -q https://github.com/openai/CLIP.git /tmp/pip-req-build-z8wsjlpn\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (4.64.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (1.12.1+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (0.13.1+cu113)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy->clip==1.0) (0.2.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->clip==1.0) (4.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (2.23.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (1.21.6)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->clip==1.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->clip==1.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->clip==1.0) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->clip==1.0) (1.24.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqE3Fj5-uYSR",
        "cellView": "form"
      },
      "source": [
        "#@title Drive Downloader\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "download_with_pydrive = True #@param {type:\"boolean\"}  \n",
        "\n",
        "class Downloader(object):\n",
        "    def __init__(self, use_pydrive):\n",
        "        self.use_pydrive = use_pydrive\n",
        "\n",
        "        if self.use_pydrive:\n",
        "            self.authenticate()\n",
        "        \n",
        "    def authenticate(self):\n",
        "        auth.authenticate_user()\n",
        "        gauth = GoogleAuth()\n",
        "        gauth.credentials = GoogleCredentials.get_application_default()\n",
        "        self.drive = GoogleDrive(gauth)\n",
        "    \n",
        "    def download_file(self, file_id, file_dst):\n",
        "        if self.use_pydrive:\n",
        "            downloaded = self.drive.CreateFile({'id':file_id})\n",
        "            downloaded.FetchMetadata(fetch_all=True)\n",
        "            downloaded.GetContentFile(file_dst)\n",
        "        else:\n",
        "            !gdown --id $file_id -O $file_dst\n",
        "\n",
        "downloader = Downloader(download_with_pydrive)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OArDkm_24w4L"
      },
      "source": [
        "#@title Imports\n",
        "\n",
        "import clip\n",
        "import os\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as nnf\n",
        "import sys\n",
        "from typing import Tuple, List, Union, Optional\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
        "from tqdm import tqdm, trange\n",
        "from google.colab import files\n",
        "import skimage.io as io\n",
        "import PIL.Image\n",
        "from IPython.display import Image \n",
        "\n",
        "\n",
        "N = type(None)\n",
        "V = np.array\n",
        "ARRAY = np.ndarray\n",
        "ARRAYS = Union[Tuple[ARRAY, ...], List[ARRAY]]\n",
        "VS = Union[Tuple[V, ...], List[V]]\n",
        "VN = Union[V, N]\n",
        "VNS = Union[VS, N]\n",
        "T = torch.Tensor\n",
        "TS = Union[Tuple[T, ...], List[T]]\n",
        "TN = Optional[T]\n",
        "TNS = Union[Tuple[TN, ...], List[TN]]\n",
        "TSN = Optional[TS]\n",
        "TA = Union[T, ARRAY]\n",
        "\n",
        "\n",
        "D = torch.device\n",
        "CPU = torch.device('cpu')\n",
        "\n",
        "\n",
        "def get_device(device_id: int) -> D:\n",
        "    if not torch.cuda.is_available():\n",
        "        return CPU\n",
        "    device_id = min(torch.cuda.device_count() - 1, device_id)\n",
        "    return torch.device(f'cuda:{device_id}')\n",
        "\n",
        "\n",
        "CUDA = get_device\n",
        "\n",
        "current_directory = os.getcwd()\n",
        "save_path = os.path.join(os.path.dirname(current_directory), \"pretrained_models\")\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "model_path = os.path.join(save_path, 'model_wieghts.pt')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ClW2ebek8DK"
      },
      "source": [
        "#@title Model\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def forward(self, x: T) -> T:\n",
        "        return self.model(x)\n",
        "\n",
        "    def __init__(self, sizes: Tuple[int, ...], bias=True, act=nn.Tanh):\n",
        "        super(MLP, self).__init__()\n",
        "        layers = []\n",
        "        for i in range(len(sizes) -1):\n",
        "            layers.append(nn.Linear(sizes[i], sizes[i + 1], bias=bias))\n",
        "            if i < len(sizes) - 2:\n",
        "                layers.append(act())\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "class ClipCaptionModel(nn.Module):\n",
        "\n",
        "    #@functools.lru_cache #FIXME\n",
        "    def get_dummy_token(self, batch_size: int, device: D) -> T:\n",
        "        return torch.zeros(batch_size, self.prefix_length, dtype=torch.int64, device=device)\n",
        "\n",
        "    def forward(self, tokens: T, prefix: T, mask: Optional[T] = None, labels: Optional[T] = None):\n",
        "        embedding_text = self.gpt.transformer.wte(tokens)\n",
        "        prefix_projections = self.clip_project(prefix).view(-1, self.prefix_length, self.gpt_embedding_size)\n",
        "        #print(embedding_text.size()) #torch.Size([5, 67, 768])\n",
        "        #print(prefix_projections.size()) #torch.Size([5, 1, 768])\n",
        "        embedding_cat = torch.cat((prefix_projections, embedding_text), dim=1)\n",
        "        if labels is not None:\n",
        "            dummy_token = self.get_dummy_token(tokens.shape[0], tokens.device)\n",
        "            labels = torch.cat((dummy_token, tokens), dim=1)\n",
        "        out = self.gpt(inputs_embeds=embedding_cat, labels=labels, attention_mask=mask)\n",
        "        return out\n",
        "\n",
        "    def __init__(self, prefix_length: int, prefix_size: int = 512):\n",
        "        super(ClipCaptionModel, self).__init__()\n",
        "        self.prefix_length = prefix_length\n",
        "        self.gpt = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "        self.gpt_embedding_size = self.gpt.transformer.wte.weight.shape[1]\n",
        "        if prefix_length > 10:  # not enough memory\n",
        "            self.clip_project = nn.Linear(prefix_size, self.gpt_embedding_size * prefix_length)\n",
        "        else:\n",
        "            self.clip_project = MLP((prefix_size, (self.gpt_embedding_size * prefix_length) // 2, self.gpt_embedding_size * prefix_length))\n",
        "\n",
        "\n",
        "class ClipCaptionPrefix(ClipCaptionModel):\n",
        "\n",
        "    def parameters(self, recurse: bool = True):\n",
        "        return self.clip_project.parameters()\n",
        "\n",
        "    def train(self, mode: bool = True):\n",
        "        super(ClipCaptionPrefix, self).train(mode)\n",
        "        self.gpt.eval()\n",
        "        return self"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7xocT3TUgey"
      },
      "source": [
        "#@title Caption prediction\n",
        "\n",
        "def generate_beam(model, tokenizer, beam_size: int = 5, prompt=None, embed=None,\n",
        "                  entry_length=67, temperature=1., stop_token: str = '.'):\n",
        "\n",
        "    model.eval()\n",
        "    stop_token_index = tokenizer.encode(stop_token)[0]\n",
        "    tokens = None\n",
        "    scores = None\n",
        "    device = next(model.parameters()).device\n",
        "    seq_lengths = torch.ones(beam_size, device=device)\n",
        "    is_stopped = torch.zeros(beam_size, device=device, dtype=torch.bool)\n",
        "    with torch.no_grad():\n",
        "        if embed is not None:\n",
        "            generated = embed\n",
        "        else:\n",
        "            if tokens is None:\n",
        "                tokens = torch.tensor(tokenizer.encode(prompt))\n",
        "                tokens = tokens.unsqueeze(0).to(device)\n",
        "                generated = model.gpt.transformer.wte(tokens)\n",
        "        for i in range(entry_length):\n",
        "            outputs = model.gpt(inputs_embeds=generated)\n",
        "            logits = outputs.logits\n",
        "            logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
        "            logits = logits.softmax(-1).log()\n",
        "            if scores is None:\n",
        "                scores, next_tokens = logits.topk(beam_size, -1)\n",
        "                generated = generated.expand(beam_size, *generated.shape[1:])\n",
        "                next_tokens, scores = next_tokens.permute(1, 0), scores.squeeze(0)\n",
        "                if tokens is None:\n",
        "                    tokens = next_tokens\n",
        "                else:\n",
        "                    tokens = tokens.expand(beam_size, *tokens.shape[1:])\n",
        "                    tokens = torch.cat((tokens, next_tokens), dim=1)\n",
        "            else:\n",
        "                logits[is_stopped] = -float(np.inf)\n",
        "                logits[is_stopped, 0] = 0\n",
        "                scores_sum = scores[:, None] + logits\n",
        "                seq_lengths[~is_stopped] += 1\n",
        "                scores_sum_average = scores_sum / seq_lengths[:, None]\n",
        "                scores_sum_average, next_tokens = scores_sum_average.view(-1).topk(beam_size, -1)\n",
        "                next_tokens_source = next_tokens // scores_sum.shape[1]\n",
        "                seq_lengths = seq_lengths[next_tokens_source]\n",
        "                next_tokens = next_tokens % scores_sum.shape[1]\n",
        "                next_tokens = next_tokens.unsqueeze(1)\n",
        "                tokens = tokens[next_tokens_source]\n",
        "                tokens = torch.cat((tokens, next_tokens), dim=1)\n",
        "                generated = generated[next_tokens_source]\n",
        "                scores = scores_sum_average * seq_lengths\n",
        "                is_stopped = is_stopped[next_tokens_source]\n",
        "            next_token_embed = model.gpt.transformer.wte(next_tokens.squeeze()).view(generated.shape[0], 1, -1)\n",
        "            generated = torch.cat((generated, next_token_embed), dim=1)\n",
        "            is_stopped = is_stopped + next_tokens.eq(stop_token_index).squeeze()\n",
        "            if is_stopped.all():\n",
        "                break\n",
        "    scores = scores / seq_lengths\n",
        "    output_list = tokens.cpu().numpy()\n",
        "    output_texts = [tokenizer.decode(output[:int(length)]) for output, length in zip(output_list, seq_lengths)]\n",
        "    order = scores.argsort(descending=True)\n",
        "    output_texts = [output_texts[i] for i in order]\n",
        "    return output_texts\n",
        "\n",
        "\n",
        "def generate2(\n",
        "        model,\n",
        "        tokenizer,\n",
        "        tokens=None,\n",
        "        prompt=None,\n",
        "        embed=None,\n",
        "        entry_count=1,\n",
        "        entry_length=67,  # maximum number of words\n",
        "        top_p=0.8,\n",
        "        temperature=1.,\n",
        "        stop_token: str = '.',\n",
        "):\n",
        "    model.eval()\n",
        "    generated_num = 0\n",
        "    generated_list = []\n",
        "    stop_token_index = tokenizer.encode(stop_token)[0]\n",
        "    filter_value = -float(\"Inf\")\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for entry_idx in trange(entry_count):\n",
        "            if embed is not None:\n",
        "                generated = embed\n",
        "            else:\n",
        "                if tokens is None:\n",
        "                    tokens = torch.tensor(tokenizer.encode(prompt))\n",
        "                    tokens = tokens.unsqueeze(0).to(device)\n",
        "\n",
        "                generated = model.gpt.transformer.wte(tokens)\n",
        "\n",
        "            for i in range(entry_length):\n",
        "\n",
        "                outputs = model.gpt(inputs_embeds=generated)\n",
        "                logits = outputs.logits\n",
        "                logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
        "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "                cumulative_probs = torch.cumsum(nnf.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "                sorted_indices_to_remove = cumulative_probs > top_p\n",
        "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[\n",
        "                                                    ..., :-1\n",
        "                                                    ].clone()\n",
        "                sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "                logits[:, indices_to_remove] = filter_value\n",
        "                next_token = torch.argmax(logits, -1).unsqueeze(0)\n",
        "                next_token_embed = model.gpt.transformer.wte(next_token)\n",
        "                if tokens is None:\n",
        "                    tokens = next_token\n",
        "                else:\n",
        "                    tokens = torch.cat((tokens, next_token), dim=1)\n",
        "                generated = torch.cat((generated, next_token_embed), dim=1)\n",
        "                if stop_token_index == next_token.item():\n",
        "                    break\n",
        "\n",
        "            output_list = list(tokens.squeeze().cpu().numpy())\n",
        "            output_text = tokenizer.decode(output_list)\n",
        "            generated_list.append(output_text)\n",
        "\n",
        "    return generated_list[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xE-uUStuv1Nl"
      },
      "source": [
        "#@title Choose pretrained model - COCO or Coneptual captions\n",
        "\n",
        "\n",
        "pretrained_model = 'Conceptual captions'  # @param ['COCO', 'Conceptual captions']\n",
        "\n",
        "if pretrained_model == 'Conceptual captions':\n",
        "  downloader.download_file(\"14pXWwB4Zm82rsDdvbGguLfx9F8aM7ovT\", model_path)\n",
        "else:\n",
        "  downloader.download_file(\"1IdaBtMSvtyzF0ByVaBHtvM0JYSXRExRX\", model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lCgFHSgr_ny"
      },
      "source": [
        "#@title GPU/CPU\n",
        "\n",
        "\n",
        "is_gpu = True #@param {type:\"boolean\"}  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bi_2zQ3QD57"
      },
      "source": [
        "#@title CLIP model + GPT2 tokenizer\n",
        "\n",
        "device = CUDA(0) if is_gpu else \"cpu\"\n",
        "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glBzYsgIwhwF"
      },
      "source": [
        "#@title Load model weights\n",
        "\n",
        "\n",
        "prefix_length = 10\n",
        "\n",
        "model = ClipCaptionModel(prefix_length)\n",
        "\n",
        "model.load_state_dict(torch.load(model_path, map_location=CPU)) \n",
        "\n",
        "model = model.eval() \n",
        "device = CUDA(0) if is_gpu else \"cpu\"\n",
        "model = model.to(device)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'frame'+ images[0] +'.png'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "pMTlAqKusTxd",
        "outputId": "54aa9830-e242-41a2-8bfb-ef08face9ce7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'frame157.png'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pohtQ8AfWNk_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "outputId": "0b314e42-82b1-4b91-e55d-f70a9c872a40"
      },
      "source": [
        "'''#@title Or download random samples form COCO test set (Karpathy et al. split)\n",
        "\n",
        "IMAGE_NAME = '354533'  # @param ['562207', '579664', '060623', '165547', '334321', '483108', '386164', '354533']\n",
        "\n",
        "name_ = \"COCO_val2014_000000\" + IMAGE_NAME + \".jpg\"\n",
        "images_path = os.path.join(os.path.dirname(current_directory), \"images\")\n",
        "os.makedirs(images_path, exist_ok=True)\n",
        "UPLOADED_FILE = os.path.join(images_path, name_)\n",
        "\n",
        "if not os.path.isfile(UPLOADED_FILE):\n",
        "  download_path = os.path.join(images_path, \"images.zip\")\n",
        "  downloader.download_file(\"1BwJeBME-dpwcCT8IXYeWz7uaPkbexjNB\", download_path)\n",
        "\n",
        "  !unzip {download_path} -d {images_path}\n",
        "\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'#@title Or download random samples form COCO test set (Karpathy et al. split)\\n\\nIMAGE_NAME = \\'354533\\'  # @param [\\'562207\\', \\'579664\\', \\'060623\\', \\'165547\\', \\'334321\\', \\'483108\\', \\'386164\\', \\'354533\\']\\n\\nname_ = \"COCO_val2014_000000\" + IMAGE_NAME + \".jpg\"\\nimages_path = os.path.join(os.path.dirname(current_directory), \"images\")\\nos.makedirs(images_path, exist_ok=True)\\nUPLOADED_FILE = os.path.join(images_path, name_)\\n\\nif not os.path.isfile(UPLOADED_FILE):\\n  download_path = os.path.join(images_path, \"images.zip\")\\n  downloader.download_file(\"1BwJeBME-dpwcCT8IXYeWz7uaPkbexjNB\", download_path)\\n\\n  !unzip {download_path} -d {images_path}\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyVkuZ07llSC"
      },
      "source": [
        "Conceptual captions examples:\n",
        "https://drive.google.com/file/d/1mzH3b0LQrGEWjEva4hI6HE_fIYRIgtBT/view?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRmcYnEfSMc_"
      },
      "source": [
        "#@title Inference\n",
        "def creating_sentence(UPLOADED_FILE):\n",
        "  use_beam_search = False #@param {type:\"boolean\"}  \n",
        "\n",
        "  image = io.imread(UPLOADED_FILE)\n",
        "  pil_image = PIL.Image.fromarray(image)\n",
        "  #pil_img = Image(filename=UPLOADED_FILE)\n",
        "  '''display(pil_image)'''\n",
        "\n",
        "  image = preprocess(pil_image).unsqueeze(0).to(device)\n",
        "  with torch.no_grad():\n",
        "      # if type(model) is ClipCaptionE2E:\n",
        "      #     prefix_embed = model.forward_image(image)\n",
        "      # else:\n",
        "      prefix = clip_model.encode_image(image).to(device, dtype=torch.float32)\n",
        "      prefix_embed = model.clip_project(prefix).reshape(1, prefix_length, -1)\n",
        "  if use_beam_search:\n",
        "      generated_text_prefix = generate_beam(model, tokenizer, embed=prefix_embed)[0]\n",
        "  else:\n",
        "      generated_text_prefix = generate2(model, tokenizer, embed=prefix_embed)\n",
        "\n",
        "  return(generated_text_prefix)\n",
        "  print('\\n')\n",
        "  print(generated_text_prefix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(diccionario)\n",
        "videoSentences = []\n",
        "for i in images:\n",
        "  file_image = 'frame'+ i +'.png'\n",
        "  file_image2 = 'frame'+ i \n",
        "  print('frame'+ i +'.png')\n",
        "  diccionario[file_image2].append(creating_sentence(file_image))\n",
        "  videoSentences.append(creating_sentence(file_image))\n",
        "videoSentences\n",
        "print(diccionario)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KasGbfPIvu10",
        "outputId": "2023eaad-7555-494e-92ff-a2a8262959ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'frame157': ['00:00:05:230000'], 'frame362': ['00:00:12:070000'], 'frame516': ['00:00:17:200000'], 'frame569': ['00:00:18:970000']}\n",
            "frame157.png\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.27it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "frame362.png\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.22it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "frame516.png\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.78it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "frame569.png\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.78it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'frame157': ['00:00:05:230000', 'person, a teacher, gives a lesson to his class.'], 'frame362': ['00:00:12:070000', 'person, a teacher, teaches students.'], 'frame516': ['00:00:17:200000', 'person, left, and person, right, are among the students who were suspended from the school.'], 'frame569': ['00:00:18:970000', 'person talks to a class of high school students.']}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keys = diccionario.keys()\n",
        "keys = list(keys)\n",
        "a= []\n",
        "start = \"00:00:00.00\"\n",
        "for i in range(len(diccionario)):\n",
        "    b= []\n",
        "    b.append(i)\n",
        "    b.append(f\"{start} --> {diccionario[keys[i]][0]}\")\n",
        "    b.append(f\"{diccionario[keys[i]][1]}\")\n",
        "    b.append(\"\")\n",
        "    start = diccionario[keys[i]][0]\n",
        "    a.append(b)\n",
        "print(a)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJliza6i8JD5",
        "outputId": "9458bb84-bd4e-41a7-a51e-e8570cfc098d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0, '00:00:00.00 --> 00:00:05:230000', 'person, a teacher, gives a lesson to his class.', ''], [1, '00:00:05:230000 --> 00:00:12:070000', 'person, a teacher, teaches students.', ''], [2, '00:00:12:070000 --> 00:00:17:200000', 'person, left, and person, right, are among the students who were suspended from the school.', ''], [3, '00:00:17:200000 --> 00:00:18:970000', 'person talks to a class of high school students.', '']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "with open('test.srt', 'w') as filehandle:\n",
        "    for i in a:\n",
        "        for j in i:\n",
        "            filehandle.write(f'{j}\\n')\n"
      ],
      "metadata": {
        "id": "18gAke2Z2PHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summarization"
      ],
      "metadata": {
        "id": "ENlJ78N9psnC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test = [\n",
        "    'person, a teacher, gives a lesson to his class.',\n",
        "    'There are 2 people sleeping '\n",
        "    'There are 3 people raising hand '\n",
        "    'person, a teacher, teaches students.', \n",
        "    'There are 1 students sleeping in the classroom'\n",
        "    'There are 2 students raising hand in the classroom'\n",
        "    'person, left, and person, right, are among the students who were suspended from the school.', \n",
        "    'person talks to a class of high school students.'\n",
        "    'There are 2 people sleeping '\n",
        "    'There are 3 people raising hand '\n",
        "    ]"
      ],
      "metadata": {
        "id": "9irvdh0V0eLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = ' '.join(videoSentences)"
      ],
      "metadata": {
        "id": "GWnMQRxOqIJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text)"
      ],
      "metadata": {
        "id": "JZlZ4Eiap1RI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75076eaf-bdce-4b57-9adb-1e0c2a747019"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "person, a teacher, gives a lesson to his class. There are 2 people sleeping There are 3 people raising hand person, a teacher, teaches students. There are 1 students sleeping in the classroomThere are 2 students raising hand in the classroomperson, left, and person, right, are among the students who were suspended from the school. person talks to a class of high school students.There are 2 people sleeping There are 3 people raising hand \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.summarization.summarizer import summarize\n",
        "print(summarize(text, word_count=30))"
      ],
      "metadata": {
        "id": "_8N8CIgxxTrr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4ec0c46-7df2-4b4c-f1d3-52618a0ee36d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.summarization.summarizer:Input text is expected to have at least 10 sentences.\n",
            "WARNING:gensim.summarization.summarizer:Input corpus is expected to have at least 10 documents.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 2 people sleeping There are 3 people raising hand person, a teacher, teaches students.\n",
            "person talks to a class of high school students.There are 2 people sleeping There are 3 people raising hand \n"
          ]
        }
      ]
    }
  ]
}